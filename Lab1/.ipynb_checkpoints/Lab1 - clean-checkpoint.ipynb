{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-29T12:46:39.707893Z",
     "start_time": "2017-03-29T15:46:39.432880+03:00"
    },
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "from os import listdir\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Displaying images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-29T12:57:00.504688Z",
     "start_time": "2017-03-29T15:57:00.493847+03:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def imshow (img):\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "def imlshow(img):\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.imshow(img, cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    \n",
    "def load_images():\n",
    "    images = []\n",
    "    img_list = listdir(\"./img\")\n",
    "    img_list.sort()\n",
    "    for img_str in img_list:\n",
    "        images.append(cv2.imread(\"img/\"+img_str))\n",
    "    return images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-29T12:46:50.128689Z",
     "start_time": "2017-03-29T15:46:50.123769+03:00"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BIN_MEAN = cv2.ADAPTIVE_THRESH_MEAN_C\n",
    "BIN_GAUSS = cv2.ADAPTIVE_THRESH_GAUSSIAN_C\n",
    "\n",
    "def adapt_bin (img, bin_type, p1, p2):\n",
    "    return cv2.adaptiveThreshold(img, 255, bin_type, cv2.THRESH_BINARY, p1, p2)\n",
    "\n",
    "def otsu_gauss_bin (img, kern_size):\n",
    "    blur = cv2.GaussianBlur(img, (kern_size, kern_size), 0)\n",
    "    return cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting edges with Canny detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-29T12:47:44.986018Z",
     "start_time": "2017-03-29T15:47:44.959729+03:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_canny(test_img):\n",
    "    merge_img = np.zeros(test_img.shape[0:2], np.uint32)\n",
    "    btest_img = cv2.GaussianBlur(test_img, (3,3),0)\n",
    "    \n",
    "    counter = 0\n",
    "    for low_thresh in range(40, 181, 5):\n",
    "        for high_thresh in range(low_thresh+5, low_thresh+101, 45):\n",
    "            canny_img = cv2.Canny(test_img, low_thresh, high_thresh)\n",
    "            gblur_img = cv2.Canny(btest_img, low_thresh, high_thresh)\n",
    "            canny_img = otsu_gauss_bin(canny_img, 3)\n",
    "            gblur_img = otsu_gauss_bin(gblur_img, 3)\n",
    "            merge_img = merge_img + canny_img + gblur_img\n",
    "            counter = counter + 2\n",
    "    \n",
    "    hsv_img = cv2.cvtColor(test_img, cv2.COLOR_BGR2HSV)\n",
    "    sat_img = hsv_img[:,:,1]\n",
    "    bsat_img = cv2.GaussianBlur(sat_img, (3,3),0)\n",
    "    val_img = hsv_img[:,:,2]\n",
    "    \n",
    "    for low_thresh in range(40, 201, 5):\n",
    "        for high_thresh in range(low_thresh+5, low_thresh+101, 45):\n",
    "            canny_sat = cv2.Canny(sat_img, low_thresh, high_thresh)\n",
    "            canny_val = cv2.Canny(val_img, low_thresh, high_thresh)\n",
    "            canny_bsat = cv2.Canny(bsat_img, low_thresh, high_thresh)\n",
    "            canny_sat = otsu_gauss_bin(canny_sat, 3)\n",
    "            canny_val = otsu_gauss_bin(canny_val, 3)\n",
    "            canny_bsat = otsu_gauss_bin(canny_bsat, 3)\n",
    "            merge_img = merge_img + canny_sat + canny_val + canny_bsat\n",
    "            counter = counter + 3\n",
    "        \n",
    "    merge_img = (merge_img / counter).astype(np.uint8)\n",
    "    merge_img = cv2.threshold(merge_img, 128, 255, cv2.THRESH_BINARY)[1]\n",
    "    cross_kern = np.array([[0,1,0],[1, 1, 1],[0, 1, 0]], np.uint8)\n",
    "    sq_kern = np.ones((3,3), np.uint8)\n",
    "    merge_img = cv2.dilate(merge_img, cross_kern)\n",
    "    merge_img = cv2.erode(merge_img, sq_kern)\n",
    "    merge_img = cv2.dilate(merge_img, sq_kern)\n",
    "    merge_img = cv2.erode(merge_img, cross_kern)\n",
    "    return merge_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting triangle shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-29T12:48:44.725240Z",
     "start_time": "2017-03-29T15:48:44.700848+03:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scal(fst, sec):\n",
    "    return fst[0]*sec[0]+fst[1]*sec[0]\n",
    "\n",
    "def norm2(fst):\n",
    "    return fst[0]**2+fst[1]**2\n",
    "\n",
    "def angle(fst, sec):\n",
    "    return scal(fst, sec)/((norm2(fst)*norm2(sec))**0.5)\n",
    "\n",
    "def check_contour(cnt):\n",
    "    leftmost   = np.asarray(cnt[cnt[:,:,0].argmin()][0], dtype=np.float32)\n",
    "    rightmost  = np.asarray(cnt[cnt[:,:,0].argmax()][0], dtype=np.float32)\n",
    "    topmost    = np.asarray(cnt[cnt[:,:,1].argmin()][0], dtype=np.float32)\n",
    "    bottommost = np.asarray(cnt[cnt[:,:,1].argmax()][0], dtype=np.float32)\n",
    "    \n",
    "    my_sides = []\n",
    "    my_sides.append(norm2(leftmost-topmost)**0.5)\n",
    "    my_sides.append(norm2(leftmost-bottommost)**0.5)\n",
    "    my_sides.append(norm2(rightmost-topmost)**0.5)\n",
    "    my_sides.append(norm2(rightmost-bottommost)**0.5)\n",
    "    my_sides.sort()\n",
    "    \n",
    "    my_diags = []\n",
    "    my_diags.append(norm2(leftmost-rightmost)**0.5)\n",
    "    my_diags.append(norm2(topmost-bottommost)**0.5)\n",
    "    my_diags.sort()\n",
    "    \n",
    "    if my_sides[1] < 60.0:  # throwing away little blobs\n",
    "         return False\n",
    "    \n",
    "    divisor = my_sides[0]+my_sides[1]  # hypothetical parts of one side\n",
    "    my_sides /= divisor  # normalising distances in sense of one side\n",
    "    my_diags /= divisor  # same here\n",
    "    \n",
    "    if (my_sides[2]-my_sides[1]-my_sides[0]) < 0.15 and \\\n",
    "       (my_sides[3]-my_sides[2]) < 0.1 and \\\n",
    "        my_diags[0] > 0.866*my_sides[2] and my_diags[0] < 1.1*my_sides[2] and \\\n",
    "        (my_diags[1]-my_sides[3])**2 < 0.01:  # max diag shouldn't differ from side size\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting centroids of chips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-29T12:49:42.251413Z",
     "start_time": "2017-03-29T15:49:42.246865+03:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean_point(fst, sec):\n",
    "    return (fst+sec)/2.0\n",
    "\n",
    "def find_centroids (contours):\n",
    "    coords = []\n",
    "    for cnt in contours:\n",
    "        M = cv2.moments(cnt)\n",
    "        cx = int(M['m10']/M['m00'])\n",
    "        cy = int(M['m01']/M['m00'])\n",
    "        coords.append([cx, cy])\n",
    "    return coords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging multiple contours into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-29T12:50:41.596652Z",
     "start_time": "2017-03-29T15:50:41.579838+03:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge_contours(contours):\n",
    "    centroids = np.asarray(find_centroids(contours))\n",
    "    delta = 10.0\n",
    "    clusters = [ [centroids[0]] ]\n",
    "    contours_to_merge = [ [contours[0]] ]\n",
    "    added = False\n",
    "    for centroid_idx, centroid_vec in enumerate(centroids[1:]):\n",
    "        for cluster_idx, cluster in enumerate(clusters):\n",
    "            for cluster_vec in cluster:\n",
    "                if norm2(centroid_vec-cluster_vec)**0.5 < delta:\n",
    "                    cluster.append(centroid_vec)\n",
    "                    contours_to_merge[cluster_idx].append(contours[centroid_idx+1])\n",
    "                    added = True\n",
    "                    break\n",
    "            if added is True:\n",
    "                break\n",
    "        \n",
    "        if added is False:\n",
    "            clusters.append([centroid_vec])\n",
    "            contours_to_merge.append([contours[centroid_idx+1]])\n",
    "        else:\n",
    "            added = False\n",
    "    \n",
    "    merged_contours = []\n",
    "    for cluster_contour in contours_to_merge:\n",
    "        merged_contours.append(cv2.convexHull(np.vstack(cluster_contour)))\n",
    "    return merged_contours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cutting image of triangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-29T12:51:19.499405Z",
     "start_time": "2017-03-29T15:51:19.487416+03:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_contour_img(img, cnt, need_to_erode=True):\n",
    "    start = (cnt[:,:,0].min(), cnt[:,:,1].min())\n",
    "    end = (cnt[:,:,0].max(), cnt[:,:,1].max())\n",
    "    mask_img = np.zeros(img.shape[0:2], np.uint8)\n",
    "    ROIpoints = cv2.approxPolyDP(cnt, 5.0, True)\n",
    "    mask_img = cv2.fillConvexPoly(mask_img, cnt, 255, 8, 0)\n",
    "    if need_to_erode is True:\n",
    "        mask_img = cv2.erode(mask_img, np.ones((9, 9), np.uint8))\n",
    "    \n",
    "    mask_img = mask_img[start[1]:end[1], start[0]:end[0]]\n",
    "    img_cut = img[start[1]:end[1], start[0]:end[0]].copy()\n",
    "    img_cut[:,:,0] = np.minimum(img_cut[:,:,0], mask_img)\n",
    "    img_cut[:,:,1] = np.minimum(img_cut[:,:,1], mask_img)\n",
    "    img_cut[:,:,2] = np.minimum(img_cut[:,:,2], mask_img)\n",
    "    \n",
    "    return img_cut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separating corners of triangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-29T12:51:54.463347Z",
     "start_time": "2017-03-29T15:51:54.448409+03:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_corners(img):\n",
    "    test_img = img.copy()\n",
    "    bin_img = make_canny(test_img)\n",
    "    cnt_img, contours, hierarchy = cv2.findContours(bin_img, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    my_contour = cv2.convexHull(np.vstack(contours)) # only one contour left \n",
    "    eps = 0.07*cv2.arcLength(my_contour,True)\n",
    "    poly_pts = cv2.approxPolyDP(my_contour, eps, True)\n",
    "    med_pts = []\n",
    "    for i in range(0, 3):\n",
    "        med_pts.append( np.array((poly_pts[i]+poly_pts[(i+1) % 3])/2.0, np.int32))\n",
    "    \n",
    "    M = cv2.moments(my_contour)\n",
    "    center = np.asarray([[int(M['m10']/M['m00']), int(M['m01']/M['m00'])]], np.int32)\n",
    "    \n",
    "    corner_images = []\n",
    "    for i in range(0, 3):\n",
    "        corner_cnt = np.array([poly_pts[i], med_pts[i], center, med_pts[(i+2)%3]], np.int32)\n",
    "        corner_images.append(get_contour_img(test_img, corner_cnt.astype(np.int32), False))\n",
    "    return corner_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building dot recognition models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-29T12:54:56.088325Z",
     "start_time": "2017-03-29T15:54:56.001050+03:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_my_models():\n",
    "    dots_data = np.asarray(pd.read_csv(\"dots.csv\"))  # sample set, that was built by myself\n",
    "    np.random.shuffle(dots_data)  # shuffling data\n",
    "\n",
    "    dots_X = dots_data[:, 1:]  # separating X and y\n",
    "    dots_y = dots_data[:, 0].ravel()\n",
    "    dots_y2 = dots_y.copy()\n",
    "    dots_y2[dots_y > 0] = 1\n",
    "    # normalising all features according to their ranges\n",
    "    dots_X[:, :3] = dots_X[:, :3]/255.0  # RGB components\n",
    "    dots_X[:,  3] = dots_X[:,  3]/360.0  # hue component (angle up to 360)\n",
    "    dots_X[:, 4:] = dots_X[:, 4:]/100.0  # saturation and value components (percentage)\n",
    "\n",
    "    dots_X2 = dots_X[dots_y > 0, :].copy()\n",
    "    dots_y3 = dots_y[dots_y > 0].copy()\n",
    "    \n",
    "    knn = KNeighborsClassifier(9)\n",
    "    knn.fit(dots_X, dots_y2)\n",
    "\n",
    "    knn2 = KNeighborsClassifier(5)\n",
    "    knn2.fit(dots_X2, dots_y3)\n",
    "    \n",
    "    return (knn, knn2)\n",
    "\n",
    "def make_dataset(img, is_xgboost):\n",
    "    #dividing img on channels\n",
    "    test_img = img.copy()\n",
    "    hsv_img = cv2.cvtColor(test_img, cv2.COLOR_BGR2HSV)\n",
    "    test_img = cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB)\n",
    "    # creating dataset and copying columns\n",
    "    dataset = np.zeros((test_img.shape[0]*test_img.shape[1], 6))\n",
    "    dataset[:,:3] = test_img.reshape((-1, 3))\n",
    "    dataset[:,3:] = hsv_img.reshape((-1, 3))\n",
    "    # normalising data\n",
    "    dataset[:, :3] = dataset[:, :3]/255.0  # RGB components\n",
    "    dataset[:,  3] = dataset[:,  3]/180.0  # hue component (angle up to 360)\n",
    "    dataset[:, 4:] = dataset[:, 4:]/255.0  # saturation and value components (percentage)\n",
    "    \n",
    "    if is_xgboost is True:\n",
    "        dataset = xgb.DMatrix( dataset)\n",
    "    return dataset\n",
    "\n",
    "def detect_dots(img, model1, model2, is_xgboost1=False, is_xgboost2=False):\n",
    "    test_img = img.copy()\n",
    "    dataset = make_dataset(test_img, is_xgboost1)\n",
    "    img_y = model1.predict(dataset)  # predicting foreground/background\n",
    "    \n",
    "    test_img = img.copy()\n",
    "    img_y = img_y.reshape((test_img.shape[0], test_img.shape[1]))\n",
    "    test_img[img_y == 0.0] = [0, 0, 0]\n",
    "    \n",
    "    dataset = make_dataset(test_img, is_xgboost2)\n",
    "    img_y2 = model2.predict(dataset)\n",
    "    \n",
    "    test_img = img.copy()\n",
    "    img_y2 = img_y2.reshape((test_img.shape[0], test_img.shape[1]))\n",
    "    test_img[img_y2 == 1.0] = [255, 255, 255]\n",
    "    test_img[img_y2 == 2.0] = [0, 255, 0]\n",
    "    test_img[img_y2 == 3.0] = [0, 255, 255]\n",
    "    test_img[img_y2 == 4.0] = [255, 255, 0]\n",
    "    test_img[img_y2 == 5.0] = [0, 0, 255]\n",
    "    test_img[img_y == 0.0] = [0, 0, 0]\n",
    "    \n",
    "    cross_kern = np.asarray([[0, 1, 0], [1, 1, 1], [0, 1, 0]], np.uint8)\n",
    "    test_img = cv2.morphologyEx(test_img, cv2.MORPH_OPEN, cross_kern)\n",
    "    test_img = cv2.morphologyEx(test_img, cv2.MORPH_CLOSE, np.ones((3, 3), np.uint8))\n",
    "    test_img = cv2.morphologyEx(test_img, cv2.MORPH_OPEN, cross_kern)\n",
    "    test_img = cv2.morphologyEx(test_img, cv2.MORPH_ERODE, np.ones((3, 3), np.uint8))\n",
    "    return test_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting dots in each corner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-29T12:55:49.558040Z",
     "start_time": "2017-03-29T15:55:49.541637+03:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_dots(img):\n",
    "    clusters = [0 for i in range(5)]\n",
    "    clus_vals = [np.array([255, 255, 255], np.uint8),\n",
    "                 np.array([0, 255, 0], np.uint8),\n",
    "                 np.array([0, 255, 255], np.uint8),\n",
    "                 np.array([255, 255, 0], np.uint8),\n",
    "                 np.array([0, 0, 255], np.uint8)]\n",
    "    back_val = np.array([0, 0, 0], np.uint8)\n",
    "    for row in img:\n",
    "        for dot in row:\n",
    "            if (dot == back_val).all():\n",
    "                continue\n",
    "            for i, clus_val in enumerate(clus_vals):\n",
    "                if (dot == clus_val).all():\n",
    "                    clusters[i] += 1\n",
    "\n",
    "    #defining patterns\n",
    "    m = max(clusters)\n",
    "    if m == 0:\n",
    "        return 0\n",
    "    elif (clusters[3] >= clusters[0] or clusters[1] >= clusters[0])\\\n",
    "         and clusters[0] > 1 and clusters[2] < clusters[0] and\\\n",
    "        clusters[4] < clusters[0]:\n",
    "        return 1\n",
    "    \n",
    "    return ([i for i, j in enumerate(clusters) if j >= m][0]+1)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-29T12:57:35.312132Z",
     "start_time": "2017-03-29T15:57:35.295027+03:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def detect_chips(img):\n",
    "    # detecting triangles\n",
    "    test_img = img.copy()\n",
    "    bin_img = make_canny(test_img)\n",
    "    c_img, contours, h = cv2.findContours(bin_img, cv2.RETR_TREE,\\\n",
    "                                          cv2.CHAIN_APPROX_SIMPLE)\n",
    "    new_contours = []\n",
    "    for contour in contours:\n",
    "        if check_contour(contour) is True:\n",
    "            new_contours.append(contour)\n",
    "\n",
    "    m_contours = merge_contours(new_contours)\n",
    "    centroids = find_centroids(m_contours)\n",
    "    \n",
    "    #detecting dots\n",
    "    cnt_images = []\n",
    "    for cnt in m_contours:\n",
    "        cnt_img = get_contour_img(test_img, cnt)\n",
    "        cnt_images.append(cnt_img)\n",
    "    \n",
    "    crn_images = []\n",
    "    for cnt_img in cnt_images:\n",
    "        crn_images.append(get_corners(cnt_img))\n",
    "        \n",
    "    model1, model2 = train_my_models()\n",
    "    \n",
    "    all_dots = []\n",
    "    for idx, crn_list in enumerate(crn_images):\n",
    "        dots_list = []\n",
    "        for crn_img in crn_list:\n",
    "            counted = count_dots(detect_dots(crn_img, model1, model2))\n",
    "            dots_list.append(counted)\n",
    "        dots_list.sort()\n",
    "        all_dots.append(dots_list)\n",
    "        \n",
    "    return (len(centroids), list(zip(centroids, all_dots)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-29T13:03:34.282130Z",
     "start_time": "2017-03-29T16:03:30.186917+03:00"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "images = load_images()\n",
    "output = detect_chips(images[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-29T13:07:31.483421Z",
     "start_time": "2017-03-29T16:07:31.479383+03:00"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open(\"out.txt\", \"w\")\n",
    "print(output[0], file=f) # N\n",
    "for entry in output[1]:\n",
    "    print(entry[0][0],\", \",entry[0][1],\"; \",entry[1][0],\", \",entry[1][1],\", \",entry[1][2], sep=\"\", file=f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
